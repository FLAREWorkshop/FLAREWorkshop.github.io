---
layout: about
title: home
permalink: /
subtitle: <b>F</b>undamental principles of <b>L</b>earning <b>A</b>nd <b>RE</b>presentation<span>&#58;</span> from Brains to LLMs.

# profile:
#   image: banner.jpeg
#   image_circular: false # crops the image to make it circular
#   # more_info: >
#   #   <p> fr.cagnetta<span style="color:gray">[at]</span>gmail.com</p>

announcements:
  enabled: False # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

# latest_posts:
#   enabled: true
#   scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
#   limit: 3 # leave blank to include all the blog posts
---

{% include figure.liquid loading="eager" path="assets/img/banner.jpeg" title="example image" class="img-fluid rounded z-depth-1" %}

---

Today, the progress of AI capabilities far outpaces Moore's law. More concerningly, it also exceeds the pace at which we understand how these systems learn, reason, and their reliability. AI is rapidly permeating many facets of society, including scientific research. Fundamental research in AI is therefore essential—not only to ensure its safe and sustainable development, but also to effectively integrate it as a scientific tool.

One of the most remarkable recent achievements in AI is the advent of Large Language Models (LLMs), like ChatGPT. These generative language models are based on deep transformer architectures pre-trained on massive text corpora. They learn to produce grammatically coherent text solely from examples—a capability that many linguists once deemed impossible. Identifying the textual correlations these models exploit, understanding the emergence of hierarchical language representation, and how these representations encode grammar and semantics are among the central questions of our time. This workshop will explore these questions guided by foundational theories and the development of synthetic, structured data models, as well as taking inspiration from the study of language as a human neuro-cognitive system.

---

## when and where
* **Date:** 11-13 May, 2026
* **Venue:** Bernoulli Center @ EPFL, Lausanne, Switzerland
* **Participation** is free, but places are limited. A registration form will be available soon.

---

## confirmed speakers

Emmanuel Abbe (EPFL)<br>
Jacob Andreas (MIT)\
Yasaman Bahri (DeepMind)\
Maissam Barkeshli (Maryland)\
Gemma Boleda (Pompeu Fabra)\
Blake Bordelon (Harvard)\
Antoine Bosselut (EPFL)\
Tankut Can (Emory)\
Emily Cheng (Pompeu Fabra)\
Alessandro Favero (Cambridge)\
Michael Gastpar (EPFL)\
Federica Gerace (UniBO)\
Surbhi Goel (UPenn)\
Jennifer Hu (Johns Hopkins)\
Jean-Rémi King (ENS / META AI)\
Alessandro Laio (SISSA)\
Ekdeep Singh Lubana (Goodfire AI)\
William Merrill (Ai2 / TTIC)\
Eshaan Nichani (Princeton)\
Isabel Papadimitriou (British Columbia)\
Andrew Saxe (UCL)\
Martin Schrimpf (EPFL)\
Greta Tuckute (Harvard)

---

## organizers

<div class="row row-cols-2 projects pt-3 pb-3">
  {% include people_horizontal.html name="Marco Baroni" affiliation="Pompeu Fabra" url="https://marcobaroni.org/" img="assets/img/organisers/mb.png" %}
  {% include people_horizontal.html name="Francesco Cagnetta" affiliation="SISSA" url="https://fracagnetta.github.io/" img="assets/img/organisers/fc.png" %}
  {% include people_horizontal.html name="Andrey Gromov" affiliation="Meta FAIR" url="https://sites.google.com/view/andreygromov/pi" img="assets/img/organisers/ag.png" %}
  {% include people_horizontal.html name="Clément Hongler" affiliation="EPFL" url="https://hongler.org/" img="assets/img/organisers/ch.png" %}
  {% include people_horizontal.html name="Matthieu Wyart" affiliation="Johns Hopkins / EPFL" url="https://physics-astronomy.jhu.edu/directory/matthieu-wyart/" img="assets/img/organisers/mw.png" %}
  {% include people_horizontal.html name="Corinne Weibel" affiliation="EPFL" url="https://people.epfl.ch/corinne.weibel?lang=en" img="assets/img/organisers/cw.png" %}
</div>

---
<!-- Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->
